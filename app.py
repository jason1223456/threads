#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import time
import random
import requests
import psycopg
from psycopg.rows import dict_row
from flask import Flask
from apscheduler.schedulers.background import BackgroundScheduler
from datetime import datetime, timedelta, timezone
import smtplib
from email.mime.text import MIMEText
from email.header import Header

# =======================================================
# âœ… ä½ åªè¦å¡«é€™è£¡ï¼ˆæŠŠä½ åŸæœ¬çš„å€¼è²¼é€²ä¾†ï¼‰
# =======================================================
API_TOKEN = "bscU4YK22+OYofSoh105OuVJZAh4tsYWZhKawi7WKjY="

DATABASE_URL = ( "postgresql://root:" "L2em9nY8K4PcxCuXV60tf1Hs5MG7j3Oz" "@sfo1.clusters.zeabur.com:30599/zeabur" )

SMTP_USER = "jason91082500@gmail.com" 
SMTP_PASS = "rwundvtaybzrgzlz" 
SMTP_TO = "leona@brainmax-marketing.com"

# =======================================================
# å›ºå®šè¨­å®š
# =======================================================
API_DOMAIN = "https://api.threadslytics.com/v1"
HEADERS = {"Authorization": f"Bearer {API_TOKEN}"}
TAIPEI_OFFSET = timedelta(hours=8)

# requests timeout
# connect timeout å›ºå®š 10sï¼›read timeout ä¾æ­¤è®Šæ•¸
REQ_TIMEOUT = 60

# =======================================================
# Lazy DB connectionï¼ˆé¿å… gunicorn import å°±é€£ DB çˆ†æ‰ï¼‰
# =======================================================
_conn = None
_cursor = None

def get_db():
    global _conn, _cursor
    if _conn is not None and _cursor is not None:
        return _conn, _cursor

    _conn = psycopg.connect(DATABASE_URL, row_factory=dict_row)
    _cursor = _conn.cursor()
    return _conn, _cursor

# =======================================================
# Gmail
# =======================================================
def send_email(subject, body):
    # è‹¥ä½ æš«æ™‚ä¸æƒ³å¯„ä¿¡ï¼šæŠŠ SMTP_* ç•™ç©ºå³å¯ï¼Œè‡ªå‹•è·³é
    if not (SMTP_USER and SMTP_PASS and SMTP_TO):
        print("â„¹ï¸ SMTP not set, skip email")
        return

    try:
        msg = MIMEText(body, "plain", "utf-8")
        msg["Subject"] = Header(subject, "utf-8")
        msg["From"] = SMTP_USER
        msg["To"] = SMTP_TO

        with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
            server.login(SMTP_USER, SMTP_PASS)
            server.sendmail(SMTP_USER, SMTP_TO, msg.as_string())

        print("ğŸ“§ Email å·²é€å‡º")
    except Exception as e:
        print("âŒ Email å¯„é€å¤±æ•—ï¼š", e)

# =======================================================
# HTTP / APIï¼ˆå¼·åŒ–ï¼šretry + backoff + jitter + æœ€å¾Œä¸ raiseï¼‰
# =======================================================
session = requests.Session()

def api_get_json(url, params=None, retries=5):
    """
    - timeout åˆ†æˆ connect/read
    - é‡è©¦ 5 æ¬¡ + æŒ‡æ•¸é€€é¿ + æŠ–å‹•
    - æœ€çµ‚å¤±æ•—å›å‚³ Noneï¼ˆä¸è¦ raiseï¼Œé¿å… APScheduler job æ•´å€‹ç‚¸æ‰ï¼‰
    """
    for i in range(1, retries + 1):
        try:
            r = session.get(
                url,
                headers=HEADERS,
                params=params,
                timeout=(10, REQ_TIMEOUT),  # (connect, read)
            )
            r.raise_for_status()
            return r.json()
        except Exception as e:
            # 1,2,4,8,16 ç§’ + jitterï¼ˆæœ€å¤š 60 ç§’ï¼‰
            base = min(60, 2 ** (i - 1))
            jitter = random.uniform(0, 1.0)
            sleep_s = base + jitter
            print(f"âš ï¸ API error retry {i}/{retries}: {e} (sleep {sleep_s:.1f}s)")
            time.sleep(sleep_s)

    print(f"âŒ API failed after {retries} retries: {url}")
    return None

def get_keyword_groups():
    data = api_get_json(f"{API_DOMAIN}/keyword-groups")
    if not data or "data" not in data:
        return []
    return data["data"]

def get_posts_by_group(group_id):
    posts = []
    page = 1
    while True:
        data = api_get_json(
            f"{API_DOMAIN}/keyword-groups/analytics/{group_id}",
            params={"metricDays": 7, "page": page},
        )
        if not data:
            break

        chunk = data.get("posts", [])
        if not chunk:
            break

        posts.extend(chunk)
        page += 1
    return posts

def get_metrics(code):
    data = api_get_json(
        f"{API_DOMAIN}/threads/post/metrics",
        params={"code": code},
    )
    if not data:
        return []
    return data.get("data", [])

# =======================================================
# METRICS
# =======================================================
def normalize_metrics(m):
    return {
        "likeCount": m.get("likeCount") or 0,
        "directReplyCount": m.get("directReplyCount") or 0,
        "shares": m.get("shares") or 0,
        "repostCount": m.get("repostCount") or 0
    }

def pick_best_metrics(metrics):
    if not metrics:
        return {"likeCount": 0, "directReplyCount": 0, "shares": 0, "repostCount": 0}
    for m in metrics:
        nm = normalize_metrics(m)
        if any(nm.values()):
            return nm
    return normalize_metrics(metrics[0])

# =======================================================
# DB: events onlyï¼ˆé‡è¦ï¼šä½ å·²æŠŠ post_time æ”¹æˆ dateï¼Œæ‰€ä»¥ç”¨ dateï¼‰
#      ä¸”ä½ çš„è¡¨æ²’æœ‰ channelï¼Œæ‰€ä»¥ä¸å¯« channel
# =======================================================
def upsert_event(post, group_name, metrics):
    try:
        conn, cursor = get_db()

        post_utc = datetime.fromisoformat(post["postCreatedAt"].replace("Z", "+00:00"))
        post_tw = (post_utc + TAIPEI_OFFSET).replace(tzinfo=None)
        now_tw = (datetime.utcnow() + TAIPEI_OFFSET).replace(tzinfo=None)

        cursor.execute("""
            INSERT INTO social_posts_events (
                date, permalink, code,
                keyword_group, keyword,
                poster_name, content, threads_topic,
                threads_like_count, threads_comment_count,
                threads_share_count, threads_repost_count,
                site, api_source,
                created_at, updated_at
            )
            VALUES (
                %s, %s, %s,
                %s, %s,
                %s, %s, %s,
                %s, %s, %s, %s,
                'THREADS', 'threadslytics',
                %s, %s
            )
            ON CONFLICT (permalink, keyword_group, keyword)
            DO UPDATE SET
                poster_name = EXCLUDED.poster_name,
                content = EXCLUDED.content,
                threads_topic = EXCLUDED.threads_topic,
                threads_like_count = EXCLUDED.threads_like_count,
                threads_comment_count = EXCLUDED.threads_comment_count,
                threads_share_count = EXCLUDED.threads_share_count,
                threads_repost_count = EXCLUDED.threads_repost_count,
                updated_at = EXCLUDED.updated_at
        """, (
            post_tw, post.get("permalink"), post.get("code"),
            group_name, post.get("keywordText"),
            post.get("username"), post.get("caption"), post.get("tagHeader"),
            metrics["likeCount"], metrics["directReplyCount"],
            metrics["shares"], metrics["repostCount"],
            now_tw, now_tw
        ))

        conn.commit()
        return "event_upsert"

    except Exception as e:
        print("DB Error (social_posts_events):", e)
        try:
            conn, _ = get_db()
            conn.rollback()
        except:
            pass
        return "skip"

# =======================================================
# JOB: æ‰‹å‹•åŒ¯å…¥ï¼ˆå‰ 10 ç­†ï¼‰
# =======================================================
def manual_import_10_events_only():
    print("\n===== ğŸš€ æ‰‹å‹•åŒ¯å…¥ 10 ç­†ï¼ˆevents onlyï¼‰ =====")
    total = 0

    groups = get_keyword_groups()
    if not groups:
        print("âš ï¸ get_keyword_groups() empty. Skip manual import.")
        return

    stats = {}
    for group in groups:
        gname = group.get("groupName", "æœªçŸ¥ç¾¤çµ„")
        posts = get_posts_by_group(group["id"])
        if not posts:
            continue

        for p in posts:
            if total >= 10:
                break

            metrics = pick_best_metrics(get_metrics(p.get("code")))
            result = upsert_event(p, gname, metrics)

            if gname not in stats:
                stats[gname] = {"upsert": 0, "total": 0}

            if result == "event_upsert":
                stats[gname]["upsert"] += 1
                stats[gname]["total"] += 1

            total += 1

        if total >= 10:
            break

    lines = ["ã€Threads æ‰‹å‹•åŒ¯å…¥å‰ 10 ç­†ï¼ˆevents onlyï¼‰ã€‘\n"]
    for g, s in stats.items():
        lines.append(f"ğŸ” é—œéµå­—ç¾¤çµ„ï¼š{g}")
        lines.append(f"ğŸ“Œ å¯«å…¥äº‹ä»¶æ•¸ï¼š{s['total']}")
        lines.append(f"ğŸ†™ Upsertï¼š{s['upsert']}\n")

    send_email("Threads æ‰‹å‹•åŒ¯å…¥æ‘˜è¦ï¼ˆevents onlyï¼‰", "\n".join(lines))

# =======================================================
# JOB: æ¯å°æ™‚åŒ¯å…¥ï¼ˆå‰ 3ï½2 å°æ™‚ï¼‰
# =======================================================
def job_import_last_2_to_3_hours_events_only():
    print("\n===== â° æ¯å°æ™‚ Threads åŒ¯å…¥ï¼ˆevents onlyï¼‰ =====")

    now = datetime.now(timezone.utc)
    start = now - timedelta(hours=3)
    end = now - timedelta(hours=2)

    start_tw = (start + TAIPEI_OFFSET).replace(tzinfo=None)
    end_tw = (end + TAIPEI_OFFSET).replace(tzinfo=None)

    lines = [
        f"æ™‚é–“å€é–“ï¼š{start_tw.strftime('%Y-%m-%d %H:%M:%S')} ï½ {end_tw.strftime('%Y-%m-%d %H:%M:%S')}\n"
    ]

    groups = get_keyword_groups()
    if not groups:
        print("âš ï¸ get_keyword_groups() empty (API unstable). Skip this run.")
        return

    for group in groups:
        gname = group.get("groupName", "æœªçŸ¥ç¾¤çµ„")
        posts = get_posts_by_group(group["id"])
        if not posts:
            continue

        stat = {"upsert": 0, "total": 0}

        for p in posts:
            # postCreatedAt æ˜¯ UTC
            try:
                t = datetime.fromisoformat(p["postCreatedAt"].replace("Z", "+00:00"))
            except Exception:
                continue

            if not (start <= t < end):
                continue

            metrics = pick_best_metrics(get_metrics(p.get("code")))
            result = upsert_event(p, gname, metrics)

            if result == "event_upsert":
                stat["upsert"] += 1
                stat["total"] += 1

        if stat["total"] == 0:
            continue

        # âœ… ä½ ä¹‹å‰æƒ³è¦ã€Œè·‘å®Œä¸€å€‹ç¾¤çµ„é€šçŸ¥ä¸€ä¸‹ã€ï¼šé€™è¡Œå°±æœ‰
        print(f"âœ… Group done: {gname} | events={stat['total']}")

        lines.append(f"ğŸ” é—œéµå­—ç¾¤çµ„ï¼š{gname}")
        lines.append(f"ğŸ“Œ æ™‚æ®µå…§äº‹ä»¶æ•¸ï¼š{stat['total']}")
        lines.append(f"ğŸ†™ Upsertï¼š{stat['upsert']}\n")

    send_email("Threads æ¯å°æ™‚åŒ¯å…¥æ‘˜è¦ï¼ˆevents onlyï¼‰", "\n".join(lines))

# =======================================================
# Flask + Schedulerï¼ˆæ”¾åœ¨ create_app è£¡ï¼Œé¿å… import å°±å•Ÿå‹•ï¼‰
# =======================================================
def create_app():
    app = Flask(__name__)

    scheduler = BackgroundScheduler()
    scheduler.add_job(job_import_last_2_to_3_hours_events_only, "cron", minute=0)
    scheduler.add_job(manual_import_10_events_only, "date", run_date=datetime.utcnow() + timedelta(seconds=5))
    scheduler.start()

    @app.route("/health")
    def health():
        # DB å£æ‰ä¹Ÿä¸è¦è®“æœå‹™èµ·ä¸ä¾†
        try:
            conn, cursor = get_db()
            cursor.execute("SELECT 1;")
            return "OK", 200
        except Exception as e:
            return f"DB_NOT_READY: {e}", 200

    @app.route("/")
    def index():
        return "Threads Events Importer Running"

    return app

# gunicorn å…¥å£
app = create_app()

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
